{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Theoretical part\n",
    "### Theoretical part\n",
    "\n",
    "1a P(mth experiment gives significant result | m experiments lacking power to reject H0) = $$\\alpha (1-\\alpha)^{m-1}$$ <br>\n",
    "1b P(at least one significant result | m experiments lacking power to reject H0) = $$\\displaystyle \\sum_{i=1}^m \\alpha (1-\\alpha)^{i-1}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experimental part"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1\n",
    "Simulate Rankings of Relevance for E and P"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-59-3ebf27a0add8>, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-59-3ebf27a0add8>\"\u001b[0;36m, line \u001b[0;32m2\u001b[0m\n\u001b[0;31m    ath, random, itertools, numpy as np\u001b[0m\n\u001b[0m                                   ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "import math, random, itertools, numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of pairs: 59049\n",
      "first 10 pairs:\n",
      "(('N', 'N', 'N', 'N', 'N'), ('N', 'N', 'N', 'N', 'N'))\n",
      "(('N', 'N', 'N', 'N', 'N'), ('N', 'N', 'N', 'N', 'R'))\n",
      "(('N', 'N', 'N', 'N', 'N'), ('N', 'N', 'N', 'N', 'HR'))\n",
      "(('N', 'N', 'N', 'N', 'N'), ('N', 'N', 'N', 'R', 'N'))\n",
      "(('N', 'N', 'N', 'N', 'N'), ('N', 'N', 'N', 'R', 'R'))\n",
      "(('N', 'N', 'N', 'N', 'N'), ('N', 'N', 'N', 'R', 'HR'))\n",
      "(('N', 'N', 'N', 'N', 'N'), ('N', 'N', 'N', 'HR', 'N'))\n",
      "(('N', 'N', 'N', 'N', 'N'), ('N', 'N', 'N', 'HR', 'R'))\n",
      "(('N', 'N', 'N', 'N', 'N'), ('N', 'N', 'N', 'HR', 'HR'))\n",
      "(('N', 'N', 'N', 'N', 'N'), ('N', 'N', 'R', 'N', 'N'))\n"
     ]
    }
   ],
   "source": [
    "def pair_generator():\n",
    "    \"\"\"\n",
    "    A generator that returns pairs of all possible combinations \n",
    "    of [N, R, HR] of length 5.\"\"\"\n",
    "    for p in itertools.product(itertools.product(['N', 'R', 'HR'], repeat=5), repeat=2):\n",
    "        yield p\n",
    "\n",
    "def random_sample(length):\n",
    "    '''\n",
    "    Returns a sample pair that\n",
    "    consists of a production and \n",
    "    an experiment list, with as possible\n",
    "    values {N, R, HR}.\n",
    "    '''\n",
    "    values = ['N', 'R', 'HR']\n",
    "    \n",
    "    p = [values[random.randint(0, 2)] for _ in range(length)]\n",
    "    e = [values[random.randint(0, 2)] for _ in range(length)]\n",
    "    \n",
    "    return p, e\n",
    "\n",
    "print('Total number of pairs:', len(list(pair_generator())))\n",
    "print('first 10 pairs:')\n",
    "pair_gen = pair_generator()\n",
    "for _ in range(10):\n",
    "    print(next(pair_gen))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2\n",
    "Implement Evaluation Measures. <br>\n",
    "Used measures: binary precision, ndcg, err"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement Evaluation Measures\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def binary_precision(documents, **kwargs):\n",
    "    \"\"\"\n",
    "    Returns the binary precision for a list of documents, which is the amount\n",
    "    of documents which where some form of relevant or !not relevant.\n",
    "    \n",
    "    \n",
    "    Keyword arguments:\n",
    "    documents -- list of document relevances\n",
    "    \"\"\"\n",
    "    return np.array([0 if d == 'N' else 1 for d in documents]).sum() / len(documents)\n",
    "\n",
    "def normalized_discounted_cumulative_gain(documents, **kwargs):\n",
    "    \"\"\"\n",
    "    Returns ndcg score of a list of documents, as decribed in http://doi.acm.org/10.1145/1645953.1646033.\n",
    "    \n",
    "    Keyword arguments:\n",
    "    documents     -- list of document relevances\n",
    "    relevance_map -- dictionary that maps relevance to score\n",
    "    \"\"\"\n",
    "    dcg = np.array([\n",
    "        (2 ** kwargs['relevance_map'][relevance] - 1) / np.log2(rank + 1)\n",
    "        for rank, relevance in enumerate(documents, start=1) \n",
    "    ])\n",
    "    return (dcg / (max(kwargs['relevance_map'].values()) * len(documents))).sum() if dcg.max() != 0 else 0\n",
    "\n",
    "def expected_reciprocal_rank(documents, **kwargs):\n",
    "    \"\"\" \n",
    "    Returns expected reciprocal rank score for list of documents, as decribed in \n",
    "    http://doi.acm.org/10.1145/1645953.1646033.\n",
    "    \n",
    "    Keyword arguments:\n",
    "    documents     -- list of document relevances\n",
    "    relevance_map -- dictionary that maps relevance to score\n",
    "    \"\"\"\n",
    "    P = 1\n",
    "    E = 0\n",
    "    for rank, relevance in enumerate(documents, start=1):\n",
    "        R = (2 ** kwargs['relevance_map'][relevance] - 1) / (2 ** max(kwargs['relevance_map'].values()))\n",
    "        E += P * R / rank\n",
    "        P *= (1-R)\n",
    "    return E"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision of production algorithm:\t 0.4\n",
      "Precision of experimental algorithm:\t 0.2\n",
      "EER of production algorithm:\t\t 0.3125\n",
      "EER of experimental algorithm:\t\t 0.375\n",
      "NDCG of production algorithm:\t\t 0.15\n",
      "NDCG of experimental algorithm:\t\t 0.189278926071\n"
     ]
    }
   ],
   "source": [
    "# Test each implemented measure on a sample datapoint\n",
    "\n",
    "production, experimental = random_sample(5)\n",
    "\n",
    "relevance_map = {\n",
    "    'N': 0,\n",
    "    'R': 1,\n",
    "    'HR': 2\n",
    "}\n",
    "\n",
    "print('Precision of production algorithm:\\t', binary_precision(documents=production))\n",
    "print('Precision of experimental algorithm:\\t', binary_precision(documents=experimental))\n",
    "\n",
    "print('EER of production algorithm:\\t\\t', expected_reciprocal_rank(\n",
    "    documents=production, relevance_map=relevance_map))\n",
    "print('EER of experimental algorithm:\\t\\t', expected_reciprocal_rank(\n",
    "    documents=experimental, relevance_map=relevance_map))\n",
    "\n",
    "print('NDCG of production algorithm:\\t\\t', normalized_discounted_cumulative_gain(\n",
    "    documents=production, relevance_map=relevance_map))\n",
    "print('NDCG of experimental algorithm:\\t\\t', normalized_discounted_cumulative_gain(\n",
    "    documents=experimental, relevance_map=relevance_map))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3\n",
    "For the three measures and all P and E ranking pairs constructed above calculate the difference: ð›¥measure = measureE-measureP. Consider only those pairs for which E outperforms P.\n",
    "\n",
    "Delta measures are calculated on 5000 random datapoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average of ndcg delta for each pair where E outperforms P\t\t 0.204116622629\n",
      "Average of err delta for each pair where E outperforms P\t\t 0.2659021336686211\n",
      "Average of binary_precision delta for each pair where E outperforms P\t 0.313307984791\n"
     ]
    }
   ],
   "source": [
    "def delta(data, eval_function, relevance_map):\n",
    "    \"\"\"\n",
    "    Returns list of document pairs in data with there corresponding \n",
    "    difference of performance per pair calculated with eval_function.\n",
    "    \n",
    "    Keyword arguments:\n",
    "    data          -- list of documents pairs containing relevance labels\n",
    "    eval_function -- evaluation function to score performance of list\n",
    "                     of relevance labels\n",
    "    relevance_map -- dictionary that contains mapping from relevance \n",
    "                     label to score\n",
    "    \"\"\"\n",
    "    return [(docs1, docs2, \n",
    "             eval_function(docs2, relevance_map=relevance_map) -\n",
    "             eval_function(docs1, relevance_map=relevance_map))\n",
    "            for docs1, docs2 in data]\n",
    "\n",
    "# Generate 5000 random datapoints\n",
    "data = [random_sample(5) for _ in range(5000)]\n",
    "\n",
    "# select pairs where Experimental algorithm outperforms Production algorithm\n",
    "data_ndcg = [x for x in delta(data, normalized_discounted_cumulative_gain, relevance_map) if x[2] > 0]\n",
    "print('Average of ndcg delta for each pair where E outperforms P\\t\\t', \n",
    "      sum([x[2] for x in data_ndcg]) / len(data_ndcg))\n",
    "\n",
    "data_err = [x for x in delta(data, expected_reciprocal_rank, relevance_map) if x[2] > 0]\n",
    "print('Average of err delta for each pair where E outperforms P\\t\\t', \n",
    "      sum([x[2] for x in data_err]) / len(data_err))\n",
    "\n",
    "data_bp = [x for x in delta(data, binary_precision, relevance_map) if x[2] > 0]\n",
    "print('Average of binary_precision delta for each pair where E outperforms P\\t', \n",
    "      sum([x[2] for x in data_bp]) / len(data_bp))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4\n",
    "Implement Team-Draft interleaving and Balanced interleaving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "not enough values to unpack (expected 3, got 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-155-d1aa999bb3d7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0mp_n\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me_n\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmapping\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mindex_ranking_pair\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m \u001b[0mI\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mA\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mB\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mteamdraft_interleaving\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp_n\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me_n\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m \u001b[0mlabels_i\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mmapping\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mI\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: not enough values to unpack (expected 3, got 2)"
     ]
    }
   ],
   "source": [
    "def teamdraft_interleaving(docs1, docs2):\n",
    "    team_a = set()\n",
    "    team_b = set()\n",
    "    i = []\n",
    "    pointers = []\n",
    "    \n",
    "    while len(set(docs1) - set(i)) > 0 and len(set(docs2) - set(i)):\n",
    "        if len(team_a) < len(team_b) or (len(team_a) == len(team_b) and random.random() > 0.5):\n",
    "            k = [x for x in docs1 if not x in i][0]\n",
    "            i.append(k)\n",
    "            team_a.add(k)\n",
    "            pointers.append('A')\n",
    "        else:\n",
    "            k = [x for x in docs2 if not x in i][0]\n",
    "            i.append(k)\n",
    "            team_b.add(k)\n",
    "            pointers.append('B')\n",
    "    return i, pointers\n",
    "\n",
    "def score_teamdraft_interleaving(a, b, I, pointers, clicks):\n",
    "    score = sum([1 if pointers[index[0]] == 'A' else -1 for index in np.argwhere(clicks==1)])\n",
    "    return 'T' if score == 0 else 'A' if score > 0 else 'B'\n",
    "\n",
    "def index_ranking_pair(p, e):\n",
    "    \"\"\"\n",
    "    Turn a pair (p, e) of rankings to pair of indices [[0, 1, 2, 3, 4], [5, 6, 7, 8, 9]].\n",
    "    Returns indiced pairs and mapping between indices and rankings.\n",
    "    \"\"\"\n",
    "    p_n = [x for x in range(len(p))]\n",
    "    e_n = [x + len(p) for x in range(len(p))]\n",
    "    mapping = p + e\n",
    "    return p_n, e_n, mapping\n",
    "\n",
    "p, e = random_sample(5)\n",
    "p_n, e_n, mapping = index_ranking_pair(p, e)\n",
    "\n",
    "I, A, B = teamdraft_interleaving(p_n, e_n)\n",
    "labels_i = [mapping[i] for i in I]\n",
    "\n",
    "print(A, B)\n",
    "print(p_n, e_n)\n",
    "print(I)\n",
    "print(labels_i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "def balanced_interleaving(A, B):\n",
    "    I = []\n",
    "    k_a = 0\n",
    "    k_b = 0\n",
    "    first = random.randint(0, 1) # 0 or 1\n",
    "       \n",
    "    while((k_a < len(A)) and (k_b < len(B))):\n",
    "        if k_a < k_b or k_a == k_b and first:\n",
    "            if A[k_a] not in I:\n",
    "                I.append(A[k_a])\n",
    "            k_a += 1\n",
    "        else:\n",
    "            if B[k_b] not in I:\n",
    "                I.append(B[k_b])\n",
    "            k_b += 1\n",
    "            \n",
    "    return np.array(I), None\n",
    "        \n",
    "def score_balanced_interleaving(A, B, I, pointers, clicks):\n",
    "    '''\n",
    "    Return the winner of algorithm A and B\n",
    "    given the interleaved list I and the\n",
    "    clicks of the user on that list.\n",
    "    '''\n",
    "    score_A = 0\n",
    "    score_B = 0\n",
    "    click_indices = np.argwhere(clicks == 1)\n",
    "    if click_indices.any(): # User has at least one click\n",
    "        last_item = I[click_indices[-1][0]]\n",
    "        playfield = highest_index(last_item, A, B)+1  \n",
    "    else:\n",
    "        return 'T' # return Tie\n",
    "    A_reduced = A[:playfield]\n",
    "    B_reduced = B[:playfield]\n",
    "\n",
    "    for index, clicked in enumerate(clicks):\n",
    "        if clicked:\n",
    "            if index in A_reduced:\n",
    "                score_A += 1\n",
    "            if index in B_reduced:\n",
    "                score_B += 1\n",
    "      \n",
    "    print(score_A, score_B)\n",
    "    if score_A > score_B:\n",
    "        return 'A'\n",
    "    elif score_B > score_A:\n",
    "        return 'B'\n",
    "    else:\n",
    "        return 'T'\n",
    "\n",
    "# Determine the line above which the items \n",
    "# of each set count towards the to be calculated score    \n",
    "def highest_index(item, A, B):\n",
    "    for index, _ in enumerate(A):\n",
    "        if item == A[index] or item == B[index]:\n",
    "            return index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5\n",
    "Implement User Clicks Simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading sessions from Yandex file...\n",
      "Number of sessions: 42652\n"
     ]
    }
   ],
   "source": [
    "# To estimate clickmdel parameters, the provided Yandex file is used\n",
    "\n",
    "def load_yandex(filename):\n",
    "    sessions = []\n",
    "    \n",
    "    with open(filename, 'r') as f:\n",
    "        data = [line.strip().split('\\t') for  line in f.readlines()]\n",
    "    for i, query_line in enumerate(data):\n",
    "        # Q indicates start of a session\n",
    "        if query_line[2] == \"Q\":\n",
    "            url_ids = query_line[5:]\n",
    "            \n",
    "            # Get url_ids of all subsequent lines that are clicks\n",
    "            clicks = np.zeros(len(url_ids))\n",
    "            for click_line in data[i+1:]:\n",
    "                if click_line[2] == \"C\":\n",
    "                    click_url = click_line[3]\n",
    "                    if click_url not in url_ids:\n",
    "                        continue\n",
    "                    clicks[url_ids.index(click_url)] = 1\n",
    "                else:\n",
    "                    break\n",
    "\n",
    "            sessions.append(clicks)\n",
    "    return np.vstack(sessions)\n",
    "\n",
    "print('Loading sessions from Yandex file...')\n",
    "sessions = load_yandex('YandexRelPredChallenge.txt')\n",
    "print('Number of sessions:', len(sessions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RCM parameters\n",
      "rho = 0.125025790115\n",
      "\n",
      "PBM parameters\n",
      "gamma = [ 0.45006096  0.1950905   0.13931351  0.10569258  0.08184845  0.06712464\n",
      "  0.05936416  0.05296352  0.04808684  0.05071275]\n"
     ]
    }
   ],
   "source": [
    "# Implement PBM and RCM\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "def calc_alpha(label):\n",
    "    return 2 ** relevance_map[label] / 2 ** max(relevance_map.values())\n",
    "\n",
    "\n",
    "class PBM(object):\n",
    "    def __init__(self, ranking_size):\n",
    "        self.alpha = defaultdict(lambda: 1)\n",
    "        self.gamma = [random.random() for _ in range(ranking_size)]\n",
    "        \n",
    "    def estimate(self, S):\n",
    "        \"\"\"Estimate parameters of the model.\"\"\"\n",
    "        self.gamma = S.sum(axis=0) / len(S)\n",
    "    \n",
    "    def predict(self, ranking):\n",
    "        \"\"\"Predict click probabilities of a given session.\"\"\"\n",
    "        return [g * calc_alpha(u) for g, u in zip(self.gamma, ranking)]\n",
    "           \n",
    "    def simulate(self, ranking):\n",
    "        \"\"\"Simulate clicks on a given session.\"\"\"\n",
    "        probs = self.predict(ranking)\n",
    "        return np.array([1 if np.random.random() < prob else 0 for prob in probs])\n",
    "\n",
    "class RCM:\n",
    "    def _init_(self):\n",
    "        self.rho = 0\n",
    "        \n",
    "    def estimate(self, S):\n",
    "        \"\"\"Estimate parameters of the model.\"\"\"\n",
    "        self.rho = np.sum(S) / S.size\n",
    "        \n",
    "    def predict(self, ranking):\n",
    "        \"\"\"Predict click probabilities of a given session.\"\"\"\n",
    "        probs = [self.rho] * len(ranking)\n",
    "        return probs\n",
    "            \n",
    "    def simulate(self, ranking):\n",
    "        \"\"\"Simulate clicks on a given session.\"\"\"\n",
    "        return np.array([1 if np.random.random() < prob else 0 for prob in self.predict(ranking)])\n",
    "\n",
    "print('RCM parameters')\n",
    "rcm_model = RCM()\n",
    "rcm_model.estimate(sessions)\n",
    "print('rho =', rcm_model.rho)\n",
    "print()\n",
    "\n",
    "print('PBM parameters')\n",
    "pbm_model = PBM(sessions.shape[1])\n",
    "pbm_model.estimate(sessions)\n",
    "print('gamma =', pbm_model.gamma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def simulate_interleaving(data, N, clickmodel, interleaver, interleave_scorer):\n",
    "    winners = []\n",
    "    for A, B, _ in data:\n",
    "        # Convert A and B to pages with unique indices for interleaving\n",
    "        a_n, b_n, mapping = index_ranking_pair(A, B)\n",
    "        # Create interleaving of indexed pages\n",
    "        I_n, pointers = interleaver(a_n, b_n)\n",
    "        # Convert indices back to rankings\n",
    "        I = [mapping[i] for i in I_n]\n",
    "#         print(A, B)\n",
    "#         print(I_n)\n",
    "#         print(I)\n",
    "        \n",
    "        # simulate clicks N times on interleaving\n",
    "        for i in range(N):\n",
    "            session = clickmodel.simulate(I)\n",
    "            winners.append(interleave_scorer(a_n, b_n, I_n, pointers, session))\n",
    "    win_percentage = winners.count('B') / (len(data) * N)\n",
    "    lose_percentage = winners.count('A') / (len(data) * N)\n",
    "    tie_percentage = winners.count('T') / (len(data) * N)\n",
    "    return win_percentage, lose_percentage, tie_percentage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "win: 0.277, lose: 0.176 tie: 0.547\n"
     ]
    }
   ],
   "source": [
    "# Simulate Teamdraft Interleaving on\n",
    "\n",
    "N = 10\n",
    "simulation = simulate_interleaving(data_err, N, pbm_model, teamdraft_interleaving, score_teamdraft_interleaving)\n",
    "print('win: {:.3f}, lose: {:.3f} tie: {:.3f}'.format(*simulation))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
