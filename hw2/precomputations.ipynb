{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def best_n_docs_per_query(n=1000):\n",
    "    '''\n",
    "    Get the best n docs per query using tfidf\n",
    "    '''\n",
    "    \n",
    "    best_docs_per_query = defaultdict(list)\n",
    "    queries_done = 0\n",
    "    duplicates = set()\n",
    "    for query_id, query_term_ids in tokenized_queries.items():\n",
    "        document_ids = itertools.chain(*[list(inverted_index[i].keys()) for i in query_term_ids])\n",
    "        scores = []\n",
    "        for doc_id in document_ids:\n",
    "            if doc_id in duplicates:\n",
    "                continue\n",
    "            score = sum(tfidf(doc_id, query_term_id) for query_term_id in query_term_ids)\n",
    "            scores.append((doc_id, str(score)))\n",
    "            duplicates.add(doc_id)\n",
    "        best_docs_per_query[query_id] = sorted(scores, key=lambda x: float(x[1]), reverse=True)[:n]\n",
    "        queries_done += 1\n",
    "        print(queries_done, \"queries run on all the documents\")\n",
    "        \n",
    "    with open('best_'+ str(n) +'_docs_per_query_no_duplicates.json', 'w') as f:\n",
    "        json.dump(best_docs_per_query, f) \n",
    "            \n",
    "    return best_docs_per_query\n",
    "\n",
    "## Create positional index with for every document all query term positions\n",
    "positional_index = {}\n",
    "for int_doc_id in range(index.document_base(), index.maximum_document()):\n",
    "    positional_index[int_doc_id] = defaultdict(list)\n",
    "    doc = index.document(int_doc_id)\n",
    "    for i, word_id in enumerate(doc[1]):\n",
    "        if word_id in query_term_ids:\n",
    "            positional_index[int_doc_id][word_id].append(i)\n",
    "\n",
    "with open('positional_index.json', 'w') as f:\n",
    "    json.dump(positional_index, f)            \n",
    "\n",
    "## Precompute all gaussian kernel values\n",
    "maximum = max(document_lengths.values())\n",
    "gaussian_kernel = {}\n",
    "maximum = max(document_lengths.values())\n",
    "for j in range(0, maximum+1):\n",
    "    gaussian_kernel[j] = {}\n",
    "    for i in range(0, maximum+1):\n",
    "         gaussian_kernel[j][i] = gaussian(j,i)\n",
    "            \n",
    "with open('gaussian_kernel.json', 'w') as f:\n",
    "        json.dump(gaussian_kernel, f)\n",
    "            \n",
    "## Precompute all gaussian cdf kernel values\n",
    "gaussian_cdf_kernel = {}\n",
    "for N in range(0, maximum+1):\n",
    "    gaussian_cdf_kernel[N] = {}\n",
    "    for i in range(0, maximum+1):\n",
    "        gaussian_cdf_kernel[N][i] = gaussian_cdf(N,i)\n",
    "    \n",
    "with open('gaussian_cdf_kernel.json', 'w') as f:\n",
    "        json.dump(gaussian_cdf_kernel, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
